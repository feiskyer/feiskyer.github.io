layout: false
---
<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
               "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head><!-- <meta name="baidu-site-verification" content="707024a76f8f40b549f07f478abab237"/> -->
<title>Building Software Systems at Google and Lessons Learned</title>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8"/>
<meta name="title" content="Building Software Systems at Google and Lessons Learned"/>
<meta name="generator" content="Org-mode"/>
<meta name="generated" content="2014-01-02T19:57+0800"/>
<meta name="author" content="dirtysalt"/>
<meta name="description" content=""/>
<meta name="keywords" content=""/>
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  html { font-family: Times, serif; font-size: 12pt; }
  .title  { text-align: center; }
  .todo   { color: red; }
  .done   { color: green; }
  .tag    { background-color: #add8e6; font-weight:normal }
  .target { }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .right  {margin-left:auto; margin-right:0px;  text-align:right;}
  .left   {margin-left:0px;  margin-right:auto; text-align:left;}
  .center {margin-left:auto; margin-right:auto; text-align:center;}
  p.verse { margin-left: 3% }
  pre {
	border: 1pt solid #AEBDCC;
	background-color: #F3F5F7;
	padding: 5pt;
	font-family: courier, monospace;
        font-size: 90%;
        overflow:auto;
  }
  table { border-collapse: collapse; }
  td, th { vertical-align: top;  }
  th.right  { text-align:center;  }
  th.left   { text-align:center;   }
  th.center { text-align:center; }
  td.right  { text-align:right;  }
  td.left   { text-align:left;   }
  td.center { text-align:center; }
  dt { font-weight: bold; }
  div.figure { padding: 0.5em; }
  div.figure p { text-align: center; }
  div.inlinetask {
    padding:10px;
    border:2px solid gray;
    margin:10px;
    background: #ffffcc;
  }
  textarea { overflow-x: auto; }
  .linenr { font-size:smaller }
  .code-highlighted {background-color:#ffff00;}
  .org-info-js_info-navigation { border-style:none; }
  #org-info-js_console-label { font-size:10px; font-weight:bold;
                               white-space:nowrap; }
  .org-info-js_search-highlight {background-color:#ffff00; color:#000000;
                                 font-weight:bold; }
  /*]]>*/-->
</style><link rel="shortcut icon" href="http://blog.com/css/favicon.ico" /> <link rel="stylesheet" type="text/css" href="./css/site.css" />


</head>
<body><!-- <div id="bdshare" class="bdshare_t bds_tools_32 get-codes-bdshare"><a class="bds_tsina"></a><span class="bds_more"></span><a class="shareCount"></a></div> --><!-- Place this tag where you want the +1 button to render --><!-- <g:plusone annotation="inline"></g:plusone> -->

<div id="preamble">

</div>

<div id="content">
<h1 class="title">Building Software Systems at Google and Lessons Learned</h1>


<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#sec-1">1 building-software-systems-at-google-and-lessons-learned.org</a>
<ul>
<li><a href="#sec-1-1">1.1 Google Web Search: 1999 vs. 2010</a>
<ul>
<li><a href="#sec-1-1-1">1.1.1 Caching in Web Search</a></li>
<li><a href="#sec-1-1-2">1.1.2 Indexing (circa 1998-1999)</a></li>
<li><a href="#sec-1-1-3">1.1.3 Early 2001: In-Memory Index</a></li>
<li><a href="#sec-1-1-4">1.1.4 Canary Requests</a></li>
<li><a href="#sec-1-1-5">1.1.5 Query Serving System, 2004 edition</a></li>
<li><a href="#sec-1-1-6">1.1.6 Encodings</a></li>
<li><a href="#sec-1-1-7">1.1.7 2007: Universal Search</a></li>
</ul>
</li>
<li><a href="#sec-1-2">1.2 System Software Evolution</a></li>
<li><a href="#sec-1-3">1.3 System Building Experiences and Patterns</a>
<ul>
<li><a href="#sec-1-3-1">1.3.1 Many Internal Services</a></li>
<li><a href="#sec-1-3-2">1.3.2 Designing Efficient Systems</a></li>
<li><a href="#sec-1-3-3">1.3.3 Designing &amp; Building Infrastructure</a></li>
<li><a href="#sec-1-3-4">1.3.4 Design for Growth</a></li>
<li><a href="#sec-1-3-5">1.3.5 Pattern: Single Master, 1000s of Workers</a></li>
<li><a href="#sec-1-3-6">1.3.6 Pattern: Tree Distribution of Requests</a></li>
<li><a href="#sec-1-3-7">1.3.7 Pattern: Backup Requests to Minimize Latency</a></li>
<li><a href="#sec-1-3-8">1.3.8 Pattern: Multiple Smaller Units per Machine</a></li>
<li><a href="#sec-1-3-9">1.3.9 Pattern: Elastic Systems</a></li>
<li><a href="#sec-1-3-10">1.3.10 Pattern: Combine Multiple Implementations</a></li>
</ul>
</li>
<li><a href="#sec-1-4">1.4 Final Thoughts</a></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-1" class="outline-2">
<h2 id="sec-1"><span class="section-number-2">1</span> building-software-systems-at-google-and-lessons-learned.org</h2>
<div class="outline-text-2" id="text-1">

<ul>
<li>author: jeff dean
</li>
<li>date: <b>从阐述的内容上看，这篇文章应该是在2010年左右写的</b>

</li>
<li>Evolution of various systems at Google 
<ul>
<li>computing hardware
</li>
<li>core search systems
</li>
<li>infrastructure software
</li>
</ul>

</li>
<li>Techniques for building large-scale systems 
<ul>
<li>decomposition into services
</li>
<li>design patterns for performance &amp; reliability
</li>
</ul>

</li>
</ul>



</div>

<div id="outline-container-1-1" class="outline-3">
<h3 id="sec-1-1"><span class="section-number-3">1.1</span> Google Web Search: 1999 vs. 2010</h3>
<div class="outline-text-3" id="text-1-1">

<ul>
<li># docs: tens of millions to tens of billions <b>~1000X</b>
</li>
<li>queries processed/day: <b>~1000X</b>
</li>
<li>per doc info in index: <b>~3X</b>
</li>
<li>update latency: months to tens of secs <b>~50000X</b> <b>这个似乎加快了不少，搜索实时性是一个很重要的问题</b> 
</li>
<li>avg. query latency: &lt;1s to &lt;0.2s <b>~5X</b>
</li>
<li>More machines * faster machines: <b>~1000X</b>
</li>
<li>Continuous evolution:
<ul>
<li>7 significant revisions in last 11 years
</li>
<li>often rolled out without users realizing we’ve made major changes
</li>
</ul>

</li>
</ul>



</div>

<div id="outline-container-1-1-1" class="outline-4">
<h4 id="sec-1-1-1"><span class="section-number-4">1.1.1</span> Caching in Web Search</h4>
<div class="outline-text-4" id="text-1-1-1">

<ul>
<li>Cache servers:
<ul>
<li>cache both index results and doc snippets
</li>
<li>hit rates typically 30-60%
<ul>
<li>depends on frequency of index updates, mix of query traffic, level of personalization, etc
</li>
</ul>

</li>
</ul>

</li>
<li>Main benefits:
<ul>
<li>performance! a few machines do work of 100s or 1000s
</li>
<li>much lower query latency on hits
<ul>
<li>queries that hit in cache tend to be both popular and expensive (common words, lots of documents to score, etc.)
</li>
</ul>

</li>
</ul>

</li>
<li>Beware: big latency spike/capacity drop when index updated or cache flushed <b>使用cache系统就必须注意cache挂掉时候带来的latency spike</b>
</li>
</ul>


<p>
<img src="./images/google-serving-system-at-1999.png"  alt="./images/google-serving-system-at-1999.png" />
</p>

</div>

</div>

<div id="outline-container-1-1-2" class="outline-4">
<h4 id="sec-1-1-2"><span class="section-number-4">1.1.2</span> Indexing (circa 1998-1999)</h4>
<div class="outline-text-4" id="text-1-1-2">

<ul>
<li>Simple batch indexing system
<ul>
<li>No real checkpointing, so machine failures painful 在做索引的时候没有checkpoint,所以机器故障会非常麻烦 
</li>
<li>No checksumming of raw data, so hardware bit errors caused problems 对于大规模数据来说，checksum还是非常必须的。
</li>
<li>Exacerbated by early machines having no ECC, no parity 
</li>
<li>Sort 1 TB of data without parity: ends up "mostly sorted" 
</li>
<li>Sort it again: "mostly sorted" another way
</li>
</ul>

</li>
<li>“Programming with adversarial memory”
<ul>
<li>Developed file abstraction that stores checksums of small records and can skip and resynchronize after corrupted records
</li>
</ul>

</li>
</ul>


<p>
<img src="./images/google-serving-system-at-1999-dealing-with-growth.png"  alt="./images/google-serving-system-at-1999-dealing-with-growth.png" />
</p>
</div>

</div>

<div id="outline-container-1-1-3" class="outline-4">
<h4 id="sec-1-1-3"><span class="section-number-4">1.1.3</span> Early 2001: In-Memory Index</h4>
<div class="outline-text-4" id="text-1-1-3">

<ul>
<li>解决index server压力的办法是通过添加更多的index replicas,但是突然有一天发现 <b>Eventually have enough replicas so that total memory across all index machines can hold ONE entire copy of index in memory</b>  replicas机器已经足够多，在内存完全可以存放下index. 然后变成如下结构
</li>
<li>Many positives:
<ul>
<li>big increase in throughput
</li>
<li>big decrease in query latency
</li>
</ul>

</li>
<li>Some issues:
<ul>
<li>Variance: query touches 1000s of machines, not dozens 
<ul>
<li><b>因为需要查询更多的机器，因此查询的变化也会非常大，不太容易控制差异</b>
</li>
<li>e.g. randomized cron jobs caused us trouble for a while
</li>
</ul>

</li>
<li>Availability: 1 or few replicas of each doc’s index data
<ul>
<li>Availability of index data when machine failed (esp for important docs): replicate important docs
</li>
<li>Queries of death that kill all the backends at once: very bad 
</li>
<li><b>存在一个问题就是，一个query可能会因为system bug造成crash. 如何解决这个问题，就是下面的canary request</b> 
</li>
</ul>

</li>
</ul>

</li>
</ul>


<p>
<img src="./images/google-serving-system-at-2001.png"  alt="./images/google-serving-system-at-2001.png" />
</p>

</div>

</div>

<div id="outline-container-1-1-4" class="outline-4">
<h4 id="sec-1-1-4"><span class="section-number-4">1.1.4</span> Canary Requests</h4>
<div class="outline-text-4" id="text-1-1-4">

<ul>
<li>主要就是针对特定query会造成system crash.但是我们不希望所有的nodes都crash. 
</li>
<li>Problem: requests sometimes cause server process to crash 
<ul>
<li>testing can help reduce probability, but can’t eliminate
</li>
</ul>

</li>
<li>If sending same or similar request to 1000s of machines:
<ul>
<li>they all might crash!
</li>
<li>recovery time for 1000s of processes pretty slow
</li>
</ul>

</li>
<li>Solution: send canary request first to one machine 可以首先将request发送到一个节点上，如果正常的话那么发送到其他节点，否则重试几次失败就放弃。 
<ul>
<li>if RPC finishes successfully, go ahead and send to all the rest
</li>
<li>if RPC fails unexpectedly, try another machine (might have just been coincidence)
</li>
<li>if fails K times, reject request
</li>
</ul>

</li>
<li>Crash only a few servers, not 1000s
</li>
</ul>


</div>

</div>

<div id="outline-container-1-1-5" class="outline-4">
<h4 id="sec-1-1-5"><span class="section-number-4">1.1.5</span> Query Serving System, 2004 edition</h4>
<div class="outline-text-4" id="text-1-1-5">

<p>我觉得这个应该是google query serving system的最终模型了。leaf所有的数据都是全内存的，但是在GFS上面有on-disk数据做持久化，或者说leaf是一个简单的query system + bigtable吧。之所有使用这种multi-level tree结构在这篇文章后面的pattern部分会说到。 
</p>
<p>
<img src="./images/google-serving-system-at-2004.png"  alt="./images/google-serving-system-at-2004.png" />
</p>
</div>

</div>

<div id="outline-container-1-1-6" class="outline-4">
<h4 id="sec-1-1-6"><span class="section-number-4">1.1.6</span> Encodings</h4>
<div class="outline-text-4" id="text-1-1-6">

<ul>
<li>Byte-Aligned Variable-length Encodings
<ul>
<li>7 bits per byte with continuation bit
<ul>
<li>Con: Decoding requires lots of branches/shifts/masks
</li>
</ul>

</li>
<li>Encode byte length using 2 bits
<ul>
<li>Better: fewer branches, shifts, and masks
</li>
<li>Con: Limited to 30-bit values, still some shifting to decode
</li>
</ul>

</li>
</ul>

</li>
<li>Group Varint Encoding
<ul>
<li>encode groups of 4 32-bit values in 5-17 bytes
</li>
<li>Pull out 4 2-bit binary lengths into single byte prefix
</li>
<li>Much faster than alternatives:
<ul>
<li>7-bit-per-byte varint: decode ~180M numbers/second
</li>
<li>30-bit Varint w/ 2-bit length: decode ~240M numbers/second 
</li>
<li>Group varint: decode ~400M numbers/second
</li>
</ul>

</li>
</ul>

</li>
</ul>


</div>

</div>

<div id="outline-container-1-1-7" class="outline-4">
<h4 id="sec-1-1-7"><span class="section-number-4">1.1.7</span> 2007: Universal Search</h4>
<div class="outline-text-4" id="text-1-1-7">

<p>从多个产品整合搜索结果，但是有下面这些问题：
</p><ul>
<li>Performance: most of the corpora weren’t designed to deal with high QPS level of web search 性能匹配 
</li>
<li>Mixing: Which corpora are relevant to query? 相关性 
</li>
<li>UI: How to organize results from different corpora? UI布局
</li>
</ul>


<p>
<img src="./images/google-universal-search-at-2007.png"  alt="./images/google-universal-search-at-2007.png" />
</p>
</div>
</div>

</div>

<div id="outline-container-1-2" class="outline-3">
<h3 id="sec-1-2"><span class="section-number-3">1.2</span> System Software Evolution</h3>
<div class="outline-text-3" id="text-1-2">

<ul>
<li>The Joys of Real Hardware (Typical first year for a new cluster):
<ul>
<li>~1 network rewiring (rolling ~5% of machines down over 2-day span)
</li>
<li>~20 rack failures (40-80 machines instantly disappear, 1-6 hours to get back)
</li>
<li>~5 racks go wonky (40-80 machines see 50% packetloss)
</li>
<li>~8 network maintenances (4 might cause ~30-minute random connectivity losses) 
</li>
<li>~12 router reloads (takes out DNS and external vips for a couple minutes)
</li>
<li>~3 router failures (have to immediately pull traffic for an hour)
</li>
<li>~dozens of minor 30-second blips for dns
</li>
<li>~1000 individual machine failures
</li>
<li>~thousands of hard drive failures
</li>
<li>slow disks, bad memory, misconfigured machines, flaky machines, etc.
</li>
<li>Long distance links: wild dogs, sharks, dead horses, drunken hunters, etc. 
</li>
<li><b>Reliability/availability must come from software!</b>
</li>
</ul>

</li>
</ul>


</div>

</div>

<div id="outline-container-1-3" class="outline-3">
<h3 id="sec-1-3"><span class="section-number-3">1.3</span> System Building Experiences and Patterns</h3>
<div class="outline-text-3" id="text-1-3">


</div>

<div id="outline-container-1-3-1" class="outline-4">
<h4 id="sec-1-3-1"><span class="section-number-4">1.3.1</span> Many Internal Services</h4>
<div class="outline-text-4" id="text-1-3-1">

<ul>
<li>Break large complex systems down into many services!
</li>
<li>Simpler from a software engineering standpoint
<ul>
<li>few dependencies, clearly specified
</li>
<li>easy to test and deploy new versions of individual services 
</li>
<li>ability to run lots of experiments
</li>
<li>easy to reimplement service without affecting clients
</li>
</ul>

</li>
<li>Development cycles largely decoupled
<ul>
<li>lots of benefits: small teams can work independently
</li>
<li>easier to have many engineering offices around the world
</li>
</ul>

</li>
<li>e.g. google.com search touches 200+ services
<ul>
<li>ads, web search, books, news, spelling correction, &hellip;
</li>
</ul>

</li>
</ul>


</div>

</div>

<div id="outline-container-1-3-2" class="outline-4">
<h4 id="sec-1-3-2"><span class="section-number-4">1.3.2</span> Designing Efficient Systems</h4>
<div class="outline-text-4" id="text-1-3-2">

<ul>
<li>Given a basic problem definition, how do you choose "best" solution?
<ul>
<li>Best might be simplest, highest performance, easiest to extend, etc.
</li>
</ul>

</li>
<li>Back of the Envelope Calculations
</li>
<li>Know Your Basic Building Blocks
<ul>
<li>Core language libraries, basic data structures, protocol buffers, GFS, BigTable, indexing systems, MapReduce, &hellip;
</li>
<li>Not just their interfaces, but understand their implementations (at least at a high level)
</li>
<li>If you don’t know what’s going on, you can’t do decent back-of-the-envelope calculations!
</li>
</ul>

</li>
</ul>


</div>

</div>

<div id="outline-container-1-3-3" class="outline-4">
<h4 id="sec-1-3-3"><span class="section-number-4">1.3.3</span> Designing &amp; Building Infrastructure</h4>
<div class="outline-text-4" id="text-1-3-3">

<ul>
<li>Identify common problems, and build software systems to address them in a general way <b>尝试从general角度解决问题，这样才能够做出infrastructure</b>
</li>
<li>Important to not try to be all things to all people <b>但是对不同需求需要不同对待，不一定需要将解决方案放在一个实现里面</b>
<ul>
<li>Clients might be demanding 8 different things
</li>
<li>Doing 6 of them is easy
</li>
<li>&hellip;handling 7 of them requires real thought
</li>
<li>&hellip;dealing with all 8 usually results in a worse system
</li>
<li>more complex, compromises other clients in trying to satisfy everyone
</li>
</ul>

</li>
<li>Don't build infrastructure just for its own sake: <b>设计通用组件的话，还需要去排除那些潜在的不需要的需求，抑制复杂性</b> 
<ul>
<li>Identify common needs and address them 
</li>
<li>Don't imagine unlikely potential needs that aren't really there
</li>
</ul>

</li>
<li>Best approach: use your own infrastructure (especially at first!) 
<ul>
<li>(much more rapid feedback about what works, what doesn't)
</li>
</ul>

</li>
</ul>


</div>

</div>

<div id="outline-container-1-3-4" class="outline-4">
<h4 id="sec-1-3-4"><span class="section-number-4">1.3.4</span> Design for Growth</h4>
<div class="outline-text-4" id="text-1-3-4">

<ul>
<li>Try to anticipate how requirements will evolve keep likely features in mind as you design base system
</li>
<li>Don’t design to scale infinitely: <b>扩展性只需要考虑5x-50x左右的扩展即可</b>
<ul>
<li>~5X - 50X growth good to consider
</li>
<li>&gt;100X probably requires rethink and rewrite
</li>
</ul>

</li>
</ul>


</div>

</div>

<div id="outline-container-1-3-5" class="outline-4">
<h4 id="sec-1-3-5"><span class="section-number-4">1.3.5</span> Pattern: Single Master, 1000s of Workers</h4>
<div class="outline-text-4" id="text-1-3-5">

<p>master主要完成全局性质的工作，其余工作交给worker完成。通常存在hot standby来做failover. 优点是可以很容易地进行全局控制，但是实现上必须小心，而缺点非常明显就是支撑worker不会很多，在1k级别上。如果涉及到更大规模集群的话，那么worker需要和master有更加频繁的交互，这对于master压力会非常大。
</p>
<ul>
<li>Master orchestrates global operation of system 
<ul>
<li>load balancing, assignment of work, reassignment when machines fail, etc.
</li>
<li>&hellip; but client interaction with master is fairly minimal
</li>
<li>Often: hot standby of master waiting to take over
</li>
<li>Always: bulk of data transfer directly between clients and workers
</li>
</ul>

</li>
<li>Examples:
<ul>
<li>GFS, BigTable, MapReduce, file transfer service, cluster scheduling system, &hellip;
</li>
</ul>

</li>
<li>Pro:
<ul>
<li>simpler to reason about state of system with centralized master
</li>
</ul>

</li>
<li>Caveats:
<ul>
<li>careful design required to keep master out of common case ops 
</li>
<li>scales to 1000s of workers, but not 100,000s of workers
</li>
</ul>

</li>
</ul>


</div>

</div>

<div id="outline-container-1-3-6" class="outline-4">
<h4 id="sec-1-3-6"><span class="section-number-4">1.3.6</span> Pattern: Tree Distribution of Requests</h4>
<div class="outline-text-4" id="text-1-3-6">

<p>这个模型本质上是从single master模型发展过来的，是multi master实现。随着master管理worker数目增加，CPU以及network IO都会bounded. 以single master为例，如果每个master最多管理1k worker的话，那么1k master可以由另外一个master管理，这样就可以支持1k * 1k worker级别了。
</p>
<ul>
<li>Problem: Single machine sending 1000s of RPCs overloads NIC on machine when handling replies
<ul>
<li>wide fan in causes TCP drops/retransmits, significant latency
</li>
<li>CPU becomes bottleneck on single machine
</li>
</ul>

</li>
<li>Solution: Use tree distribution of requests/responses
<ul>
<li>fan in at root is smaller
</li>
<li>cost of processing leaf responses spread across many parents
</li>
</ul>

</li>
<li>Most effective when parent processing can trim/combine leaf data 
<ul>
<li>can also co-locate parents on same rack as leaves
</li>
</ul>

</li>
</ul>


</div>

</div>

<div id="outline-container-1-3-7" class="outline-4">
<h4 id="sec-1-3-7"><span class="section-number-4">1.3.7</span> Pattern: Backup Requests to Minimize Latency</h4>
<div class="outline-text-4" id="text-1-3-7">

<p>通过backup request来降低延迟，因为部分请求可能会成为straggler，这点在mapreduce里面的speculative非常经典。 <b>NOTE(blog):但是jeff dean在另外一篇文章里面也说了这样也可能存在some bad case</b>
</p>
<ul>
<li>Problem: variance high when requests go to 1000s of machines
<ul>
<li>last few machines to respond stretch out latency tail substantially
</li>
</ul>

</li>
<li>Often, multiple replicas can handle same kind of request
</li>
<li>When few tasks remaining, send backup requests to other replicas 
</li>
<li>Whichever duplicate request finishes first wins
<ul>
<li>useful when variance is unrelated to specifics of request 
</li>
<li>increases overall load by a tiny percentage
</li>
<li>decreases latency tail significantly
</li>
</ul>

</li>
<li>Examples:
<ul>
<li>MapReduce backup tasks (granularity: many seconds)
</li>
<li>various query serving systems (granularity: milliseconds)
</li>
</ul>

</li>
</ul>


</div>

</div>

<div id="outline-container-1-3-8" class="outline-4">
<h4 id="sec-1-3-8"><span class="section-number-4">1.3.8</span> Pattern: Multiple Smaller Units per Machine</h4>
<div class="outline-text-4" id="text-1-3-8">

<p>每个机器上部署更小的单元，可以使得调度更加容易，集群资源利用率更高。
</p>
<ul>
<li>Problems:
<ul>
<li>want to minimize recovery time when machine crashes 
</li>
<li>want to do fine-grained load balancing
</li>
</ul>

</li>
<li>Having each machine manage 1 unit of work is inflexible
<ul>
<li>slow recovery: new replica must recover data that is O(machine state) in size
</li>
<li>load balancing much harder
</li>
</ul>

</li>
<li>Have each machine manage many smaller units of work/data 
<ul>
<li>typical: ~10-100 units/machine
</li>
<li>allows fine grained load balancing (shed or add one unit)
</li>
<li>fast recovery from failure (N machines each pick up 1 unit)
</li>
</ul>

</li>
<li>Examples:
<ul>
<li>map and reduce tasks, GFS chunks, Bigtable tablets, query serving system index shards
</li>
</ul>

</li>
</ul>


</div>

</div>

<div id="outline-container-1-3-9" class="outline-4">
<h4 id="sec-1-3-9"><span class="section-number-4">1.3.9</span> Pattern: Elastic Systems</h4>
<div class="outline-text-4" id="text-1-3-9">

<p>可伸缩的系统，自动调节整个集群资源利用率。这个东西可以打个比方，如果整个集群资源空闲的话，那么可以减少线程数目，释放一些内存让其他程序可以有效运行。而当压力比较大的时候，可以保持在一个水平不至于崩溃。 <b>TODO(blog):感觉不是很理解</b> 
</p>
<ul>
<li>Problem: Planning for exact peak load is hard
<ul>
<li>overcapacity: wasted resources 
</li>
<li>undercapacity: meltdown
</li>
</ul>

</li>
<li>Design system to adapt:
<ul>
<li>automatically shrink capacity during idle period 
</li>
<li>automatically grow capacity as load grows
</li>
</ul>

</li>
<li>Make system resilient to overload:
<ul>
<li>do something reasonable even up to 2X planned capacity
<ul>
<li>e.g. shrink size of index searched, back off to less CPU intensive algorithms, drop spelling correction tips, etc.
</li>
</ul>

</li>
<li>more aggressive load balancing when imbalance more severe
</li>
</ul>

</li>
</ul>


</div>

</div>

<div id="outline-container-1-3-10" class="outline-4">
<h4 id="sec-1-3-10"><span class="section-number-4">1.3.10</span> Pattern: Combine Multiple Implementations</h4>
<div class="outline-text-4" id="text-1-3-10">

<p>多种实现的结合，这点以realtime + batch说明非常直观。
</p>
<ul>
<li>Example: Google web search system wants all of these:
<ul>
<li>freshness (update documents in ~1 second)
</li>
<li>massive capacity (10000s of requests per second)
</li>
<li>high quality retrieval (lots of information about each document) 
</li>
<li>massive size (billions of documents)
</li>
</ul>

</li>
<li>Very difficult to accomplish in single implementation
</li>
<li>Partition problem into several subproblems with different engineering tradeoffs. E.g.
<ul>
<li>realtime system: few docs, ok to pay lots of $$$/doc 
</li>
<li>base system: high # of docs, optimized for low $/doc 
</li>
<li>realtime+base: high # of docs, fresh, low $/doc
</li>
</ul>

</li>
</ul>


</div>
</div>

</div>

<div id="outline-container-1-4" class="outline-3">
<h3 id="sec-1-4"><span class="section-number-3">1.4</span> Final Thoughts</h3>
<div class="outline-text-3" id="text-1-4">

<ul>
<li>Today: exciting collection of trends: <b>未来趋势的一些思考</b>
<ul>
<li>large-scale datacenters + 大规模数据中心建设 
</li>
<li>increasing scale and diversity of available data sets +  大量数据需要分析和挖掘 
</li>
<li>proliferation of more powerful client devices 各种设备接入 <b>NOTE（blog）：是否可以理解为移动客户端</b> 

</li>
</ul>

</li>
<li>Many interesting opportunities: <b>值得去做的事情</b>
<ul>
<li>planetary scale distributed systems 宇宙级别分布式系统
</li>
<li>development of new CPU and data intensive services 新的CPU和数据密集服务 <b>TODO(blog):???</b>
</li>
<li>new tools and techniques for constructing such systems 以及构建这些服务的工具
</li>
</ul>

</li>
</ul>

</div>
</div>
</div>
</div>

<div id="postamble">
<p class="date">Date: 2014-01-02T19:57+0800</p>
<p class="creator"><a href="http://orgmode.org">Org</a> version 7.9.2 with <a href="http://www.gnu.org/software/emacs/">Emacs</a> version 23</p>
<a href="http://validator.w3.org/check?uri=referer">Validate XHTML 1.0</a>

</div>
<!-- Baidu Analytics BEGIN --><script type="text/javascript">var _bdhmProtocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cscript src='" + _bdhmProtocol + "hm.baidu.com/h.js%3F54a700ad7035f6e485eaf2300641e7e9' type='text/javascript'%3E%3C/script%3E"));</script><!-- Baidu Analytics END --><!-- Google Analytics BEGIN --><!-- <script type="text/javascript">  var _gaq = _gaq || [];  _gaq.push(['_setAccount', 'UA-31377772-1']);  _gaq.push(['_trackPageview']);  (function() {    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);  })();</script> --><!-- Google Analytics END --><!-- Baidu Button BEGIN --><!-- <script type="text/javascript" id="bdshare_js" data="type=tools&amp;uid=6762177" ></script><script type="text/javascript" id="bdshell_js"></script><script type="text/javascript"> document.getElementById("bdshell_js").src = "http://bdimg.share.baidu.com/static/js/shell_v2.js?cdnversion=" + Math.ceil(new Date()/3600000)</script> --><!-- Baidu Button END --><!-- G+ BEGIN --><!-- Place this render call where appropriate --><!-- <script type="text/javascript">  (function() {    var po = document.createElement('script'); po.type = 'text/javascript'; po.async = true;    po.src = 'https://apis.google.com/js/plusone.js';    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);  })();</script> --><!-- G+ END --><!-- DISQUS BEGIN --><div id="disqus_thread"></div><script type="text/javascript">/* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * *//* required: replace example with your forum shortname  */var disqus_shortname = 'feiskyblog';var disqus_identifier = 'building-software-systems-at-google-and-lessons-learned.html';var disqus_title = 'building-software-systems-at-google-and-lessons-learned.html';var disqus_url = 'http://blog.com/building-software-systems-at-google-and-lessons-learned.html';/* * * DON'T EDIT BELOW THIS LINE * * */(function() {var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);})();</script><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript><a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a><!-- DISQUS END --></body>
</html>
